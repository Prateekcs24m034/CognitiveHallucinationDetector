Cognitive Framework for Intrinsic Hallucination Detection in LLMs

This project includes a report to implement a cognitively inspired approach for detecting hallucinations in Large Language Models. The framework models human reading behavior using two components: Global Attention Bias for selecting the most relevant sentence from the context and Local Attention Bias for predicting token-level saliency based on gaze patterns. These signals guide a transformer-based classifier to detect hallucinations without relying on external knowledge bases.

Features

Global Attention Bias for context sentence selection
Local Attention Bias for token importance estimation
Human gaze based training corpus
End to end BERT based hallucination classifier
