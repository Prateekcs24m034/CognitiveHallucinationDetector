# Cognitive Framework for Intrinsic Hallucination Detection in LLMs

This project includes a report to implement a cognitively inspired approach for detecting hallucinations in Large Language Models. The framework models human reading behavior using two components: Global Attention Bias for selecting the most relevant sentence and Local Attention Bias for predicting token-level saliency. These signals guide a transformer-based classifier to detect hallucinations without relying on external knowledge bases.

## Features

Global Attention Bias for context selection  
Local Attention Bias for token importance estimation  
Human gaze based training corpus  
End to end BERT based hallucination classifier  

## Structure

data for datasets  
models for GAB, LAB, and classifier modules  
scripts for preprocessing and training  
notebooks for experiments  

## Contact

For any questions or collaboration inquiries, feel free to reach out.
